---
title: "OpenAI APIï¼šå¾®è°ƒï¼ˆfine-tuningï¼‰çš„ä½¿ç”¨
date: 2023-04-01T23:30:00+08:00
tags:
- AI
---


### 1. å‡†å¤‡æ•°æ®ï¼Œå¦‚ä¸‹æ˜¯æ•°æ®ç»“æ„

```json
{"prompt": "<è®­ç»ƒçš„æç¤º><æç¤ºç»“æŸè¯ï¼Œä½œä¸ºå’Œè¡¥é½çš„åŒºåˆ†>", "completion": "<è¦è¡¥é½çš„åˆ†ç±»æˆ–è€…æè¿°><è¡¥é½ç»“æŸè¯ï¼Œé¿å…è¡¥é½æ‰©æ•£>"}
```


### 2. è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼Œä»¥åŠä¸‹ä¸€æ­¥çš„å‘½ä»¤

```
openai -k <ä½ çš„apikey> tools fine_tunes.prepare_data -f data.json -q
```

å®ƒä¼šè‡ªåŠ¨å°†åˆ†æä½ çš„æ•°æ®é›†ï¼Œæ‹†å°æˆè®­ç»ƒé›†ï¼ˆ`_prepared_train` åç¼€ï¼‰å’ŒéªŒè¯é›†ï¼ˆ`_prepared_valid` åç¼€ï¼‰

**è¾“å‡º**:

```text
Analyzing...

- Your file contains 7739 prompt-completion pairs
- Based on your data it seems like you're trying to fine-tune a model for classification
- For classification, we recommend you try one of the faster and cheaper models, such as `ada`
- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training
- More than a third of your `prompt` column/key is uppercase. Uppercase prompts tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See [https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) for more details
- All prompts end with suffix `\n\n###\n\n`
- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See [https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) for more details

Based on the analysis we will perform the following actions:
- [Recommended] Lowercase all your data in column/key `prompt` [Y/n]: Y
- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y
- [Recommended] Would you like to split into training and validation set? [Y/n]: Y


Your data will be written to a new JSONL file. Proceed [Y/n]: Y

Wrote modified files to `data_prepared_train.jsonl` and `data_prepared_valid.jsonl`
Feel free to take a look!

Now use that file when fine-tuning:
> openai api fine_tunes.create -t "data_prepared_train.jsonl" -v "data_prepared_valid.jsonl" --compute_classification_metrics --classification_n_classes 445

After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `\n\n###\n\n` for the model to start generating completions, rather than continuing with the prompt.
Once your model starts training, it'll approximately take 3.13 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.
```

### 3. å¼€å§‹è¿›è¡Œå¾®è°ƒæ¨¡å¼

```bash
openai api fine_tunes.create \
	-t "data_prepared_train.jsonl" 
	-v "data_prepared_valid.jsonl" 
	--compute_classification_metrics --classification_n_classes {åˆ†ç±»æ•°} 
```

```text
Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509k/509k [00:00<00:00, 403Mit/s]
Uploaded file from data_prepared_train.jsonl: file-ZORlJyH7tHZX7T4nSTcZsSkm
Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.4k/75.4k [00:00<00:00, 58.3Mit/s]
Uploaded file from data_prepared_valid.jsonl: file-bGVC1Rhuu9i059NOtctNB4R9
Created fine-tune: ft-HnegTA8pgKKWxVKopDss0Z7G
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-03-31 23:06:07] Created fine-tune: ft-HnegTA8pgKKWxVKopDss0Z7G

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-HnegTA8pgKKWxVKopDss0Z7G
```

#### å¦‚æœè¾“å‡ºä¸­æ–­ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢å‘½ä»¤ç»§ç»­ï¼š

```bash
openai api fine_tunes.follow -i ft-HnegTA8pgKKWxVKopDss0Z7G
```

```text
[2023-04-03 19:27:39] Created fine-tune: ft-pTUpDsM7AlHkALMgSpeQxdD2
[2023-04-03 19:32:44] Fine-tune costs $2.41
[2023-04-03 19:32:44] Fine-tune enqueued. Queue number: 0
[2023-04-03 19:32:45] Fine-tune started
[2023-04-03 19:40:52] Completed epoch 1/4
[2023-04-03 19:55:40] Completed epoch 3/4
[2023-04-03 20:03:47] Uploaded model: curie:ft-personal-2023-04-03-12-03-46
[2023-04-03 20:03:48] Uploaded result file: file-obQbEb3RxT4Xz3T9rg1V5IgY
[2023-04-03 20:03:48] Fine-tune succeeded

Job complete! Status: succeeded ğŸ‰
Try out your fine-tuned model:

openai api completions.create -m curie:ft-personal-2023-04-03-12-03-46 -p <YOUR_PROMPT>
```
