---
title: "OpenAI APIï¼šå¾®è°ƒï¼ˆfine-tuningï¼‰çš„ä½¿ç”¨
date: 2023-04-01T23:30:00+08:00
tags:
- AI
---

é€šè¿‡ [[notes/2023-03-28-openai-api-finetuning|OpenAI APIï¼šå¾®è°ƒï¼ˆfine-tuningï¼‰åŠå…¶ä½¿ç”¨åœºæ™¯]] å¯¹ Fine-Tuning çš„ä»‹ç»ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£å¸¸è§åœºæ™¯ä»¥åŠç›¸å…³åœºæ™¯çš„ä½¿ç”¨ Trickã€‚æ€»çš„æ¥è¯´ï¼Œå¾®è°ƒçš„å¥½å¤„å°±æ˜¯ ==æ›´çœé’±ï¼Œç­”æ¡ˆæ›´æ˜ç¡®==ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°±æ¥ä»‹ç»ä¸‹å…·ä½“çš„å·¥ç¨‹å®ç°ç»†èŠ‚ï¼Œä¸»è¦åˆ†æˆå››ä¸ªæ­¥éª¤æ¥å†™ï¼š

1. æ•°æ®é¢„æµ‹å¤„ç†
2. æ¨¡å‹è®­ç»ƒ
3. æ¨¡å‹è¯„ä¼°

**å¼€å§‹ä¹‹å‰ï¼Œéœ€è¦å¦‚ä¸‹å‡†å¤‡**ï¼š

- å®‰è£… **OpenAI** çš„å‘½ä»¤è¡Œå·¥å…·ï¼š`pip install --upgrade openai`
- è®¾ç½®ä½ çš„**API KEY**ï¼š`export OPENAI_API_KEY="<OPENAI_API_KEY>"`
	- å¯ä»¥ä»è¿™ä¸ªåœ°æ–¹è·å–ï¼š https://platform.openai.com/account/api-keys
	- æ³¨å†Œ OpenAI è´¦å·çš„æ—¶å€™ï¼Œä¼šå…è´¹èµ é€ 18 åˆ€çš„é¢åº¦ã€‚==æ³¨æ„ï¼Œå®ƒæ˜¯æœ‰ä½¿ç”¨æœŸé™çš„ï¼==

### 1. æ•°æ®é¢„å¤„ç†

#### æ•°æ®æ ¼å¼

**è®­ç»ƒé›†çš„æ ¼å¼**ï¼š

```json
{
  "prompt": "<è®­ç»ƒçš„æç¤º><åˆ†éš”ç¬¦>",
  "completion": " <è¦è¡¥é½çš„åˆ†ç±»æˆ–è€…æè¿°><åœæ­¢è¯>"
}
```

**æ¨¡å‹è°ƒç”¨çš„æ–¹å¼**ï¼š

```json
/* POSTÂ https://api.openai.com/v1/completions -d */
{
	"model": "<ä½ è®­ç»ƒå¥½çš„æ¨¡å‹åç§°>",
	"prompt": "<ä½ æµ‹è¯•çš„æç¤º><åˆ†éš”ç¬¦: å’Œè®­ç»ƒé›†çš„æ ¼å¼ä¸­çš„åˆ†éš”ç¬¦ä¸€è‡´>",
	"stop": ["<åœæ­¢è¯: å’Œè®­ç»ƒé›†çš„æ ¼å¼ä¸­çš„åœæ­¢è¯ä¸€è‡´>"]
	... /* å…¶ä»–å¯é€‰å‚æ•° */
}
```


- æ¯ä¸€ä¸ª**æç¤º**éƒ½åº”è¯¥ä»¥ç›¸åŒçš„**åˆ†éš”ç¬¦**ç»“å°¾ï¼Œæ¥å‘Šè¯‰æ¨¡å‹==æç¤ºç»“æŸï¼Œå¼€å§‹è¡¥é½==ï¼Œæ³¨æ„è¿™ä¸ª**åˆ†éš”ç¬¦**ä¸èƒ½å‡ºç°åœ¨**æç¤º**å’Œ**è¡¥é½**ä¸­ï¼›
- æ¯ä¸€ä¸ª**è¡¥é½**éƒ½åº”è¯¥ä»¥ç©ºç™½å­—ç¬¦å¼€å¤´(` `)ï¼Œå› ä¸ºGPTçš„**æ ‡æ³¨å™¨**æ ‡æ³¨å¤§éƒ¨åˆ†**åˆ†è¯**éƒ½ä»¥ç©ºç™½æ‰“å¤´ï¼›
- æ¯ä¸€ä¸ª**è¡¥é½**éƒ½åº”è¯¥ä»¥ç›¸åŒçš„**åœæ­¢è¯**ç»“å°¾ï¼Œæ¥å‘Šè¯‰æ¨¡å‹==è¡¥é½ç»“æŸ==ï¼›
- **æ¨ç†**çš„æ—¶å€™ï¼Œ**æç¤º**çš„æ ¼å¼åº”è¯¥è¦==å’Œè®­ç»ƒé›†çš„æ ¼å¼æ„å»ºæ–¹å¼ç›¸åŒ==ï¼š
	- **æç¤º**çš„æ–‡æœ¬ç»“æ„å’Œè®­ç»ƒé›†çš„**æç¤º**ä¸€è‡´ï¼›
	- **æç¤º**è¦ä»¥ç›¸åŒçš„**åˆ†éš”ç¬¦**ç»“æŸï¼›
	- å°† `stop` å‚æ•°å’Œè®­ç»ƒé›†çš„**è¡¥é½**çš„**åœæ­¢è¯** è®¾ç½®æˆä¸€æ ·çš„ï¼›

#### è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼Œä»¥åŠä¸‹ä¸€æ­¥çš„å‘½ä»¤

```bash
openai tools fine_tunes.prepare_data -f data.json -q
```

å®ƒä¼šè‡ªåŠ¨å°†åˆ†æä½ çš„æ•°æ®é›†ï¼Œæ‹†å°æˆè®­ç»ƒé›†ï¼ˆ`_prepared_train` åç¼€ï¼‰å’ŒéªŒè¯é›†ï¼ˆ`_prepared_valid` åç¼€ï¼‰

**è¾“å‡º**:

```text
Analyzing...

- Your file contains 7739 prompt-completion pairs
- Based on your data it seems like you're trying to fine-tune a model for classification
- For classification, we recommend you try one of the faster and cheaper models, such as `ada`
- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training
- More than a third of your `prompt` column/key is uppercase. Uppercase prompts tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See [https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) for more details
- All prompts end with suffix `\n\n###\n\n`
- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See [https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) for more details

Based on the analysis we will perform the following actions:
- [Recommended] Lowercase all your data in column/key `prompt` [Y/n]: Y
- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y
- [Recommended] Would you like to split into training and validation set? [Y/n]: Y


Your data will be written to a new JSONL file. Proceed [Y/n]: Y

Wrote modified files to `data_prepared_train.jsonl` and `data_prepared_valid.jsonl`
Feel free to take a look!

Now use that file when fine-tuning:
> openai api fine_tunes.create -t "data_prepared_train.jsonl" -v "data_prepared_valid.jsonl" --compute_classification_metrics --classification_n_classes 445

After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `\n\n###\n\n` for the model to start generating completions, rather than continuing with the prompt.
Once your model starts training, it'll approximately take 3.13 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.
```

### 2. å¾®è°ƒæ¨¡å‹çš„è®­ç»ƒ

```bash
openai api fine_tunes.create \
	-t "data_prepared_train.jsonl" 
	-v "data_prepared_valid.jsonl" 
	--compute_classification_metrics --classification_n_classes {åˆ†ç±»æ•°} 
```

```text
Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 509k/509k [00:00<00:00, 403Mit/s]
Uploaded file from data_prepared_train.jsonl: file-ZORlJyH7tHZX7T4nSTcZsSkm
Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.4k/75.4k [00:00<00:00, 58.3Mit/s]
Uploaded file from data_prepared_valid.jsonl: file-bGVC1Rhuu9i059NOtctNB4R9
Created fine-tune: ft-HnegTA8pgKKWxVKopDss0Z7G
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-03-31 23:06:07] Created fine-tune: ft-HnegTA8pgKKWxVKopDss0Z7G

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-HnegTA8pgKKWxVKopDss0Z7G
```

#### å¦‚æœè¾“å‡ºä¸­æ–­ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢å‘½ä»¤ç»§ç»­ï¼š

```bash
openai api fine_tunes.follow -i ft-HnegTA8pgKKWxVKopDss0Z7G
```

```text
[2023-04-03 19:27:39] Created fine-tune: ft-pTUpDsM7AlHkALMgSpeQxdD2
[2023-04-03 19:32:44] Fine-tune costs $2.41
[2023-04-03 19:32:44] Fine-tune enqueued. Queue number: 0
[2023-04-03 19:32:45] Fine-tune started
[2023-04-03 19:40:52] Completed epoch 1/4
[2023-04-03 19:55:40] Completed epoch 3/4
[2023-04-03 20:03:47] Uploaded model: curie:ft-personal-2023-04-03-12-03-46
[2023-04-03 20:03:48] Uploaded result file: file-obQbEb3RxT4Xz3T9rg1V5IgY
[2023-04-03 20:03:48] Fine-tune succeeded

Job complete! Status: succeeded ğŸ‰
Try out your fine-tuned model:

openai api completions.create -m curie:ft-personal-2023-04-03-12-03-46 -p <YOUR_PROMPT>
```

### 3.  å¾®ä¿¡æ¨¡å‹çš„è¯„ä¼°

```bash
openai api fine_tunes.results -i ft-pTUpDsM7AlHkALMgSpeQxdD2 > result.csv
```

#### æ•°æ®å¯è§†åŒ–

```python
import plotly.graph_objs as go
import plotly.io as pio
import pandas as pd

results = pd.read_csv('result.csv')
results[results['classification/accuracy'].notnull()].tail(1)
roc = results[results['classification/accuracy'].notnull()]['classification/accuracy'] #.plot()
# åˆ›å»ºæŸ±å½¢å›¾
fig_roc = go.Figure(data=[go.Line(x=roc.index, y=roc)])

# è®¾ç½®å¸ƒå±€å’Œæ ·å¼
fig_roc.update_layout(title='Bar chart', xaxis_title='X-axis label', yaxis_title='Y-axis label')

# æ˜¾ç¤ºå›¾å½¢
pio.show(fig_roc)
```