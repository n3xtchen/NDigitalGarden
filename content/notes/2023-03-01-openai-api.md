---
title: "OpenAI API 使用说明"
created: 2023-03-01 15:08
tags:
- AI
---

> [API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/completions)

### 请求体的格式

```json
/* POST https://api.openai.com/v1/completions -d */
/* POST https://api.openai.com/v1/chat/completions -d */
{
	"model": "text-davinci-003",
	"prompt": "请告诉我如何编写一个工作汇报",
	... /* 其他可选参数 */
}
```

### 必需参数

- **model**：需要调用模型；
	- 输入类型：字符串；
	- 例如：`text-davinci-003` ，说明我们要调用的是 **GPT3** 的模型；
- **prompt**：一个输入的文本段落或短语，作为生成模型输出的起点或引导；
	- 输入的类型：字符串或者一个字符列表；
	- 例如:
		- `"请告诉我如何编写一个工作汇报"`；
		- `["请你扮演下资深数据分析师", "帮我写一段 SQL 来发分析。。。"]`。

### 调优参数

#### 最大令牌数：max_tokens

可以简单理解为生成文本的单词（或者词组）数。

**token** 是文本的基本单位，他可以是一个单词、一个标点符号或者一个词组，取决于分词器。

**默认**：16
**最大值**：4096

==较大的 max_tokens 值会导致生成更长的文本，但也会增加生成文本的时间和计算成本。==

#### 采样方式

`temperature` 参数更加强调==随机性和多样性==，而 `top_n` 参数更加强调==准确性和流畅性==。

##### 温度采样：`temperature`

控制了生成文本的“温度”，温度越高，则生成文本的随机性和创造力也越高，但也可能导致生成的文本不那么准确或连贯。

**原理**：通过在 **softmax** 操作中对概率分布进行加权，从而使生成的文本更加随机和多样化。

**默认值**：1
**取值范围**：0.0～2.0，值越大，随机性和创造力越高，准确率和连贯性越低。

##### 核采样：`top_p`:

温度采样的替代品，其中模型考虑了具有 `top_p` 概率质量的 **token** 的结果。

**原理**：在 **softmax** 操作中选择概率值最高的前 `top_p` 个 **token**，从而限制了 **token** 选择的范围，使生成的文本更加多样化。

**默认值**: 1
**取值范围**：0.0-1.0，0.1 意味着只考虑包含前 10% 概率质量的 **token**。

##### 使用建议

==不推荐和 temperature 混用，取其一。==

通常情况下，可以先尝试将 `temperature` 和 `top_n` 参数都设置为较小的值，以获得最基本的生成结果。如果生成的文本不够随机或多样化，可以逐步增大 `temperature` 参数的值或减小 `top_n` 参数的值；如果生成的文本不够准确或流畅，可以逐步减小 `temperature` 参数的值或增大 `top_n` 参数的值。

#### 惩罚参数

`frequency_penalty` 和 `presence_penalty` 是 **OpenAI GPT** 系列模型中的两个参数，可以用于控制==生成文本中词汇的重复程度和多样性==。

##### 重复惩罚参数： `frequency_penalty`

用于惩罚在生成文本中==出现过多次==的词汇；通常建议将该参数设置在  -2.0 到 2.0 之间。

##### 出现惩罚参数：`presence_penalty`

用于惩罚在生成文本中==出现过==的词汇；通常建议将该参数设置在 -2.0 到 2.0 之间。

##### 使用建议

这两个参数的具体设置需要根据具体应用场景和需求进行调整。通常情况下，可以先尝试将 `frequency_penalty` 和 `presence_penalty` 参数都设置为 0.0，以获得最基本的生成结果。

- 如果生成的文本中存在==过多的重复词汇==，可以逐步**增大** `frequency_penalty` 参数的值；
	- 设置较大的值可以==减少文本中重复的词汇==，但也可能导致生成的文本==不够流畅==或不符合语言习惯；
- 如果生成的文本中存在==较少的词汇多样性==，可以逐步**增大** `presence_penalty` 参数的值；
	- 设置较大的值可以==增加文本中的词汇多样性==，但也可能导致生成的文本==不够准确==或不符合语言习惯。

#### 文本主题控制

##### 提前终止：stop

`stop` 用于指定模型生成文本的停止词，当模型生成的文本中包含了 `stop` 参数中指定的词语时，模型将停止生成文本。例如，设置 `stop` 参数为 `['pizza']`，则模型将在生成文本中包含 `pizza` 时停止生成，从而生成与 `pizza` 相关的文本。

==**前提条件**：`top_p` > 1==

**本质**：如果指定了某个停止词，生成内容的 token 会尽可能的接近这个停止词的含义，间接达到了==控制生成文本主题或方向==的作用。

**默认值**：null

**格式**：文本或者数组

**格式如下**：

```json
// 文本
{
	"stop": "pizza"
}
// 数组
{
	"stop": ["pizza", "delicious"]
}
```

> **为什么 `top_n` = 1 时，`stop` 会失效？**
> 如果将 `top_n` 参数设置为 1，即只允许模型输出预测概率最高的单个 token，那么设置 `stop` 参数将不会起到作用，因为模型只会考虑生成的下一个 token 是否是预测概率最高的那个 token，而不会考虑后续的 token。
> 例如，如果将 `top_p` 参数设置为 1，`stop` 参数设置为 `['pizza']`，并以 "I love to eat pizza" 为输入，那么模型会在考虑下一个单词时，只会选取概率最高的单个 token，而不会考虑该单词后面是否包含停止词 `pizza`。因此，如果需要同时使用 `stop` 和 `top_p` 参数，需要将 `top_p` 参数设置为大于 1 的数值。

##### 主题偏好调整：logit_bias

> 需要注意的是，`logit_bias` 参数==只适用于一些特定的生成任务==，例如情感分析、文本分类等任务，==对于一般的文本生成任务可能没有显著的效果==。

`logit_bias` 参数是一个包含不同标记偏好的字典，可以将该参数用于修改特定标记的生成偏好。例如，将 logit_bias 参数设置为 `{'happy': 2.0, 'sad': -2.0}`，则 API 生成的文本序列中将更有可能出现 **happy** 相关的单词，同时更少出现 **sad** 相关的单词。

**默认值**： `null`

**格式如下**：

```
{
  "token": weight
}
```

#### 二次筛选：best_of

`best_of` 参数在服务端生成 **K** 个结果并挑选最优的结果返回（），可以==用于增强生成文本的准确性和流畅度==。可以理解为 2 次召回，第一轮进行 K 个结果的粗召回，然后从这个结果中挑选出最优的序列。

**筛选原理**：选取每个 **token** 的**对数似然**（**Log Prob**）概率都是最高的那一个结果。

**默认值**：1，它的值一定要大于参数 `n`。

> 需要注意的是，较大的 best_of 参数值可能会==导致 API 响应时间延长和 API 调用次数限制增加==。

#### 优雅的结束符：suffix

`suffix` 参数可以用于指定生成文本的结尾字符串，帮助控制生成文本的长度和质量。

有三种用途：
- **优雅的结尾**，让 gpt 把语句补充完成，使==生成的文本更加完整和自然==；
	- `prompt` 是开始，`suffix` 是指定结尾，首位呼应；
	- 可以通过 `suffix` 参数添加一些额外的文本作为输出的结尾，如问候语、签名等。这样可以使生成的文本更加自然和完整；
		- 例如将 `suffix` 参数设置为 `'感谢使用'`，表示在生成的文本末尾添加一个感谢的结尾语；
- **限制不当言论**，当相关（==有争议和危险的言论==）词语出现的时候，立刻终止，避免不必要的法律风险和道德问题，作用有点像 `logit_bias` 中的权重降权，只不过他是彻底的坚强；
- **提前终止**：当模型达到指定的结尾字符串时，模型将停止生成文本并返回结果，这可以==避免生成过长或不准确的文本==。

### 功能参数

#### 返回多条文本

##### 流式响应：`stream`

用于需要==大量生成文本==时，为了==节省内存和提高程序的效率和稳定性==，而不是一次性获取所有结果。这种情况下，使用流式响应方式可以==逐个获取==每个生成的文本结果，以确保程序不会因为内存不足或处理时间过长而崩溃或出现异常情况。

以下是一些适合使用流式响应的场景：
-   批量生成大量文本数据，并逐个处理或保存每个结果；
-   为大量文本数据集生成摘要或标签，需要逐个处理每个数据项；
-   生成连续的文本序列，如对话、故事等，需要不断地追加新的文本片段，直到满足预设的条件为止。

**默认值**：`false`
**结束的标识**：`data: [DONE]`

> 需要注意的是，使用流式响应方式可能会==导致程序的运行时间变长==，因为 API 需要不断地生成新的结果并返回给客户端，而客户端需要不断地迭代处理每个结果，直到满足预设的条件为止。

##### 直接返回：`n`

在服务测生成 `n` 条结果后，一次性返回给客户端。

**默认值**：1

##### 使用建议

==`n` 和 `stream` 互斥，只能设置一个。==

==建议使用 `stop` 和 `max_tokens` 来限制生成的文本长度或持续时间，并在其中任何一个条件满足时停止生成文本。==

#### 应答：echo

- 在输出中，重新打印输入的 **prompt**；
- **默认值**：false。

### 评估参数

#### 对数似然估计：logprobs

`logprobs` 参数用于计算和评估生成文本的质量。

**默认值**：`null`, 免费的接口，最多设成 5，API 将返回5 个 log 概率值最高的 token 和它的概率值。

==使用 `logprobs` 参数可能会导致响应时间延长和 API 调用次数限制增加，因为每个 token 的概率值都需要进行计算和返回。==