---
title: "AI 入门：归一化（Normalization）"
date: 2023-04-27T16:08:00+08:00
tags:
- AI
---

在机器学习中，**归一化（Normaliztion，简写 Norm）** 是指将数据按比例缩放，使之==落入一个小的特定区间内==，便于不同特征之间的比较和权衡。常见的归一化方法有**最大最小值归一化（MinMax）**、**Z-Score标准化**、**小数定标规范化**等。

归一化的作用是将不同数量级的数据变成同一数量级，消除数量级的影响，从而消除奇异样本数据导致的不良影响， 从而加速网络的训练，提高模型的泛化能力。

### 常用的方法

#### BN（Batch Normalization）

在一个 Batch 内，对==列==进行归一化。当单个 Batch 的数量足够大的时候，均值和标准差接近全样本的值时，可以采用 **BN**，可以认为是近似全局归一化。


**内部协变量偏移(Internal Covariate Shift)**:每一层的参数在更新过程中，会改变下一层输入的分布，神经网络层数越多，表现得越明显，(就比如高层大厦底部发生了微小偏移，楼层越高，偏移越严重。)

#### LN（Layer Normalization）

对每一个样本的所有特征（即==行==），进行归一化。

**特点**：

- 每一个特征的==量纲都相同==；
- 不同样本的同一个位置的分布概率不同；
- 每一个样本的特征的==长度不一样==；（其他归一化所不拥有的特性）

例如，一个句子是一个样本，句子包含的只有分词（量纲相同）；句子中的词顺序如果发生变化了，不会改变我们对这个句子的理解；每个句子的长短自然都都不一样。

所以 **LN** 天然是为 **RNN** 而生的归一化方式，而且在实际使用过程成，确实能够效果比较明显，但是在 **CNN** 上 效果不如 **BN**（可能原因是图像不能完全忽略纵向特征的顺序）